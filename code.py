# -*- coding: utf-8 -*-
"""Копия блокнота "ЮФО.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MrTKnhZ8jBmwaObrXfOetJWNOdMfS8DJ
"""

# Commented out IPython magic to ensure Python compatibility.
# Работа с массивами данных
import numpy as np 

# Работа с таблицами
import pandas as pd

# Отрисовка графиков
import matplotlib.pyplot as plt

# Функции-утилиты для работы с категориальными данными
from tensorflow.keras import utils

# Класс для конструирования последовательной модели нейронной сети
from tensorflow.keras.models import Sequential

# Основные слои
from tensorflow.keras.layers import Dense, Dropout, Conv1D, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation,MaxPooling1D

# Токенизатор для преобразование текстов в последовательности
from tensorflow.keras.preprocessing.text import Tokenizer

# Заполнение последовательностей до определенной длины
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Матрица ошибок классификатора
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from sklearn.preprocessing import OneHotEncoder

# Кодирование тестовых меток
from sklearn.preprocessing import LabelEncoder

# Разбиение на тренировочную и тестовую выборки
from sklearn.model_selection import train_test_split

from tensorflow.keras.optimizers import Adam

# Загрузка датасетов из облака google
import gdown

# Отрисовка графиков
import matplotlib.pyplot as plt

# %matplotlib inline

# Для отрисовки графиков
import seaborn as sns
import warnings

"""## **anvil**"""

!pip install anvil-uplink

import anvil.server
anvil.server.connect("4JRGT4YFRHXVKN2E6DCZ236W-BMK4WG3FUTLLY6FJ")



"""## **Загрузка датасетов**"""

!unzip -q "SFO.zip" -d /content/ # подгрузим базу в ноутбук в директорию 'content/'

ls

xlcom = pd.ExcelFile('1. Компании.xlsx')

print(xlcom.sheet_names)

dfcom = xlcom.parse('Sheet0')

dfcom.columns = dfcom.columns.str.replace(" ", "_")
dfcom

dfcom.describe(include=[object])

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "БАЗИС"')]

dfcom[(dfcom.Подотрасль=="н/д")]

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom['Наименование_поддержанной_компании'].value_counts().head(50)

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "СПУТНИКС"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "НПО "АТОМ"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'АО "ГРУППА Т-1"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "УМНЫЙ ГОРОД"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "ТЕКСЕЛ"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "ПЕТР ТЕЛЕГИН"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "УМНЫЙ ГОРОД"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "УМНЫЙ ГОРОД"')]



dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "АРМАТЕХ"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "БИОЭНЕРДЖИ"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "СПЛАВ-АРМ"')]



dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "ТЕРЕБРА"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "МЕС-ТЕХНОЛОГИИ"')]

dfcom[(dfcom.Наименование_поддержанной_компании	== 'ООО "БАЗОВЫЕ ТЕХНОЛОГИИ"')]

bt2 = dfcom['Наименование_поддержанной_компании'].value_counts()
dfcom[dfcom['Наименование_поддержанной_компании'].isin(bt2[bt2 >= 2].index)].sort_values(['Наименование_поддержанной_компании']).head(50)

bt2 = dfcom['Наименование_поддержанной_компании'].value_counts()
dfcom[dfcom['Наименование_поддержанной_компании'].isin(bt2[bt2 >= 2].index)].sort_values(['Наименование_поддержанной_компании'])





# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Отрасль" , data  = dfcom)

len(dfcom['Подотрасль'].unique())

dfcom['Технология_(1_уровень)'].unique()

dfcom['Технология_(2_уровень)'].unique()

dfcom['Технология_(3_уровень)'].unique()

len(dfcom['Технология_(3_уровень)'].unique())



# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Отрасль.value_counts()

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Подотрасль" , data  = dfcom)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Подотрасль.value_counts().head(59)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Подотрасль.value_counts().tail(59)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология_(1_уровень)" , data  = dfcom)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология_(2_уровень)" , data  = dfcom)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология_(3_уровень)" , data  = dfcom)





xlprod = pd.ExcelFile('2. Продукты_new.xlsx')

print(xlprod.sheet_names)

dfprod = xlprod.parse('Sheet0')

dfprod.columns = dfprod.iloc[0]
dfprod = dfprod.iloc[1:]

dfprod.columns = dfprod.columns.str.replace(" ", "_")
dfprod

dfprod.describe(include=[object])

bt3 = dfprod['Компания'].value_counts()
dfprod[dfprod['Компания'].isin(bt3[bt3 >= 2].index)].sort_values(['Компания'])





cols = dfprod.columns[7:12] # первые 
# определяем цвета 
# желтый - пропущенные данные, синий - не пропущенные
colours = ['#000099', '#ffff00'] 
sns.heatmap(dfprod[cols].isnull(), cmap=sns.color_palette(colours))

dfprod[dfprod['Вид_(ОКПД_2)'].isna()]

dfprod[dfprod['Категория_(ОКПД_2)'].isna()]

dfprod[dfprod['Подкатегория_(ОКПД_2)'].isna()]



# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Раздел_(ОКПД_2)" , data  = dfprod)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Класс_(ОКПД_2)" , data  = dfprod)

dfprod[dfprod['Подкатегория_(ОКПД_2)'].isna()]











xlotr = pd.ExcelFile('3. Отрасли.xlsx')

print(xlprod.sheet_names)

dfotr = xlotr.parse('Sheet0')

dfotr

dfotr.describe(include=[object])

dfotr[(dfotr.Подотрасль=="Прочие услуги")]



# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Отрасль" , data  = dfotr)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfotr.Отрасль.value_counts()





# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Подотрасль" , data  = dfotr)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfotr.Подотрасль.value_counts().head(50)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfotr.Подотрасль.value_counts().tail(50)



xltech = pd.ExcelFile('4. Технологии.xlsx')

print(xltech.sheet_names)

dftech = xltech.parse('Sheet0')

dftech

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология (1 уровень)" , data  = dftech)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология (2 уровень)" , data  = dftech)

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Технология (3 уровень)" , data  = dftech)







xlsprotr = pd.ExcelFile('Справочник. Отрасли и подотрасли.xlsx')

print(xlsprotr.sheet_names)

dfsprotr = xlsprotr.parse('2022.04.19. Справочник отраслей')

dfsprotr.columns = dfsprotr.iloc[0]
dfsprotr = dfsprotr.iloc[1:]
dfsprotr

# Сводка по распределению данных графическое(визуальное)
warnings.simplefilter('ignore')
sns.countplot(x = "Отрасль" , data  = dfsprotr)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Отрасль.value_counts().head(50)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Подотрасль.value_counts().head(50)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
dfcom.Подотрасль.value_counts().tail(50)

xlsprtech = pd.ExcelFile('Справочник. Технологии.xlsx')

print(xlsprtech.sheet_names)

dfsprtech = xlsprtech.parse('Технологии')

dfsprtech.columns = dfsprtech.iloc[0]
dfsprtech = dfsprtech.iloc[1:]
dfsprtech

dfsprtech['3 уровень (уровень тегирования участников)'].unique()

len(dfsprtech['3 уровень (уровень тегирования участников)'].unique())

cols = dfsprtech.columns[5:8] # первые 30 колонок
# определяем цвета 
# желтый - пропущенные данные, синий - не пропущенные
colours = ['#000099', '#ffff00'] 
sns.heatmap(dfsprtech[cols].isnull(), cmap=sns.color_palette(colours))

"""## Загрузка сводной таблицы"""

df = pd.read_csv("Merged.csv", encoding='utf8')

df

df = df.rename(columns={'Технология (3 уровень)': 'Технология3'})

df = df.rename(columns={'Раздел (ОКПД 2)': 'РазделОКПД2', 'Класс (ОКПД 2)': 'КлассОКПД2', 'Подкласс (ОКПД 2)': 'ПодклассОКПД2',
       'Группа (ОКПД 2)': 'ГруппаОКПД2', 'Подгруппа (ОКПД 2)': 'ПодгруппаОКПД2', 'Вид (ОКПД 2)': 'ВидОКПД2', 'Категория (ОКПД 2)': 'КатегорияОКПД2', 
       'Подкатегория (ОКПД 2)': 'ПодкатегорияОКПД2'})





df[(df['Наименование поддержанной компании']	== 'ООО "БАЗИС"')]

df.describe(include=[object])

df[(df['Наименование поддержанной компании']	== 'ООО "ШЕРПА РОБОТИКС"')]

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
df['Наименование поддержанной компании'].value_counts().head(50)

# Сводка по распределению данных по каждой категории, здесь видим почти  кратное превышение среднего 
df['Наименование поддержанной компании'].value_counts().head(50)

# Пример данных из таблицы
print(df.values[0])





"""## ** Модель Подотрасль


"""

# Функция вывода сводки по распределению данных
def data_summary(df, Подотрасль):
    for cls in Подотрасль:
        print(f'Количество записей класса {cls}: {df[df.Подотрасль == cls].shape[0]}')

# Сводка по распределению данных
data_summary(df, df.Подотрасль.unique())

# Задание параметров преобразования
VOCAB_SIZE       = 30000                  # Объем словаря для токенизатора
TRAIN_TEST_RATIO = 0.4                    # Доля проверочной выборки в основном наборе

# Назначение интервала допустимого количества текстов по классу
text_count_interval = [7, 400]

# Фильтрация данных - отбрасываются все данные без категории
# и все классы объемом менее нижней границы разрешенного интервала
class_to_drop = ['нет данных']
for cls in df.Подотрасль.unique():
    if df[df.Подотрасль == cls].shape[0] < text_count_interval[0]:
        class_to_drop.append(cls)

print(f'Удаляются классы: {class_to_drop}')

del_mask = df.Подотрасль.isin(class_to_drop)
df = df[~del_mask]

# По всем оставшимся классам количество примеров ограничивается не более
# верхней границы разрешенного интервала
for cls in df.Подотрасль.unique():
    df = df.drop(df[df.Подотрасль == cls].index[text_count_interval[1]:])

# Извлечение всех записей-текстов обращений и их меток классов
text_data = df['Описание продукта (с указанием конкретных характеристик)'].tolist()
class_data = df.Подотрасль.tolist()

# Токенизация и построение частотного словаря по обучающим текстам
# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря
tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='неизвестное_слово', char_level=False)

# Использованы параметры:
# num_words   - объем словаря
# filters     - убираемые из текста ненужные символы
# lower       - приведение слов к нижнему регистру
# split       - разделитель слов
# char_level  - указание разделять по словам, а не по единичным символам
# oov_token   - токен для слов, которые не вошли в словарь

# Построение частотного словаря по обучающим текстам
tokenizer.fit_on_texts(text_data)

# Построение словаря в виде пар слово - индекс
items = list(tokenizer.word_index.items())

# Кодирование меток классов индексами (числами)
encoder = LabelEncoder()
class_labels = encoder.fit_transform(class_data)

CLASS_LIST = encoder.classes_
CLASS_COUNT = len(CLASS_LIST)

print(f'Размер словаря: {len(items)}')
print(f'Список классов: {CLASS_LIST}')
print(f'Всего классов: {CLASS_COUNT}')
print(f'Форма выходных данных: {class_labels.shape}')
print(f'Пример числовых меток классов: {class_labels[:10]}')

# Преобразование входных текстов в последовательности индексов (для архитектур с embedding)
# Применено приведение к массиву объектов для дальнейшего разделения на выборки
x_data = np.array(tokenizer.texts_to_sequences(text_data), dtype=object)
# Вывод формы и примера данных
print(x_data.shape) 
print(x_data[0][:20])

# Преобразование входных текстов в разреженную матрицу из векторов Bag of Words
x_data_01 = tokenizer.texts_to_matrix(text_data)
# Вывод формы и примера данных
print(x_data_01.shape) 
print(x_data_01[0, :20])

# Преобразование меток класса к векторам one hot encoding
y_data = utils.to_categorical(class_labels, CLASS_COUNT)
# Вывод формы и примера данных
print(y_data.shape)
print(y_data[0])

# Функция компиляции и обучения модели нейронной сети
def compile_train_model(model, 
                        x_train,
                        y_train,
                        x_val,
                        y_val,
                        optimizer='adam',
                        epochs=50,
                        batch_size=128,
                        figsize=(20, 5)):

    # Компиляция модели
    model.compile(optimizer=optimizer, 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])

    # Вывод сводки
    model.summary()

    # Обучение модели с заданными параметрами
    history = model.fit(x_train,
                        y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(x_val, y_val))

    # Вывод графиков точности и ошибки
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    fig.suptitle('График процесса обучения модели')
    ax1.plot(history.history['accuracy'], 
               label='Доля верных ответов на обучающем наборе')
    ax1.plot(history.history['val_accuracy'], 
               label='Доля верных ответов на проверочном наборе')
    ax1.xaxis.get_major_locator().set_params(integer=True)
    ax1.set_xlabel('Эпоха обучения')
    ax1.set_ylabel('Доля верных ответов')
    ax1.legend()

    ax2.plot(history.history['loss'], 
               label='Ошибка на обучающем наборе')
    ax2.plot(history.history['val_loss'], 
               label='Ошибка на проверочном наборе')
    ax2.xaxis.get_major_locator().set_params(integer=True)
    ax2.set_xlabel('Эпоха обучения')
    ax2.set_ylabel('Ошибка')
    ax2.legend()
    plt.show()


# Функция вывода результатов оценки модели на заданных данных
def eval_model(model, x, y_true,
               class_labels=[],
               cm_round=3,
               title='',
               figsize=(10, 10)):
    # Вычисление предсказания сети
    y_pred = model.predict(x)
    # Построение матрицы ошибок
    cm = confusion_matrix(np.argmax(y_true, axis=1),
                          np.argmax(y_pred, axis=1),
                          normalize='true')
    # Округление значений матрицы ошибок
    cm = np.around(cm, cm_round)

    # Отрисовка матрицы ошибок
    fig, ax = plt.subplots(figsize=figsize)
    ax.set_title(f'Нейросеть {title}: матрица ошибок нормализованная', fontsize=18)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
    disp.plot(ax=ax)
    plt.gca().images[-1].colorbar.remove()  # Стирание ненужной цветовой шкалы
    plt.xlabel('Предсказанные классы', fontsize=16)
    plt.ylabel('Верные классы', fontsize=16)
    fig.autofmt_xdate(rotation=45)          # Наклон меток горизонтальной оси при необходимости
    plt.show()    

    print('-'*100)
    print(f'Нейросеть: {title}')

    # Для каждого класса:
    for cls in range(len(class_labels)):
        # Определяется индекс класса с максимальным значением предсказания (уверенности)
        cls_pred = np.argmax(cm[cls])
        # Формируется сообщение о верности или неверности предсказания
        msg = 'ВЕРНО :-)' if cls_pred == cls else 'НЕВЕРНО :-('
        # Выводится текстовая информация о предсказанном классе и значении уверенности
        print('Класс: {:<20} {:3.0f}% сеть отнесла к классу {:<20} - {}'.format(class_labels[cls],
                                                                               100. * cm[cls, cls_pred],
                                                                               class_labels[cls_pred],
                                                                               msg))

    # Средняя точность распознавания определяется как среднее диагональных элементов матрицы ошибок
    print('\nСредняя точность распознавания: {:3.0f}%'.format(100. * cm.diagonal().mean()))


# Совместная функция обучения и оценки модели нейронной сети
def compile_train_eval_model(model, 
                             x_train,
                             y_train,
                             x_test,
                             y_test,
                             class_labels=CLASS_LIST,
                             title='',
                             optimizer='adam',
                             epochs=50,
                             batch_size=128,
                             graph_size=(20, 5),
                             cm_size=(10, 10)):

    # Компиляция и обучение модели на заданных параметрах
    # В качестве проверочных используются тестовые данные
    compile_train_model(model, 
                        x_train, y_train,
                        x_test, y_test,
                        optimizer=optimizer,
                        epochs=epochs,
                        batch_size=batch_size,
                        figsize=graph_size)

    # Вывод результатов оценки работы модели на тестовых данных
    eval_model(model, x_test, y_test, 
               class_labels=class_labels, 
               title=title,
               figsize=cm_size)

# Построение гистограммы распределения длин текстов основного набора
seq_len = [len(x) for x in x_data]
plt.hist(seq_len, 50)
plt.show()

data_summary(df, df.Подотрасль.unique())

# Извлечение всех записей-текстов обращений и их меток классов
text_data = df['Описание продукта (с указанием конкретных характеристик)'].tolist()
class_data = df.Подотрасль.tolist()

# Токенизация и построение частотного словаря по обучающим текстам
# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря
tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='неизвестное_слово', char_level=False)

# Использованы параметры:
# num_words   - объем словаря
# filters     - убираемые из текста ненужные символы
# lower       - приведение слов к нижнему регистру
# split       - разделитель слов
# char_level  - указание разделять по словам, а не по единичным символам
# oov_token   - токен для слов, которые не вошли в словарь

# Построение частотного словаря по обучающим текстам
tokenizer.fit_on_texts(text_data)

# Построение словаря в виде пар слово - индекс
items = list(tokenizer.word_index.items())

# Кодирование меток классов индексами (числами)
encoder = LabelEncoder()
class_labels = encoder.fit_transform(class_data)

CLASS_LIST = encoder.classes_
CLASS_COUNT = len(CLASS_LIST)

print(f'Размер словаря: {len(items)}')
print(f'Список классов: {CLASS_LIST}')
print(f'Всего классов: {CLASS_COUNT}')
print(f'Форма выходных данных: {class_labels.shape}')
print(f'Пример числовых меток классов: {class_labels[:10]}')

# Преобразование входных текстов в последовательности индексов (для архитектур с embedding)
# Применено приведение к массиву объектов для дальнейшего разделения на выборки
x_data = np.array(tokenizer.texts_to_sequences(text_data), dtype=object)
# Вывод формы и примера данных
print(x_data.shape) 
print(x_data[0][:20])

# Преобразование входных текстов в разреженную матрицу из векторов Bag of Words
x_data_01 = tokenizer.texts_to_matrix(text_data)
# Вывод формы и примера данных
print(x_data_01.shape) 
print(x_data_01[0, :20])

# Преобразование меток класса к векторам one hot encoding
y_data = utils.to_categorical(class_labels, CLASS_COUNT)
# Вывод формы и примера данных
print(y_data.shape)
print(y_data[0])

# Получение индексов разделения основного набора на обучающую и тестовую выборки
idx_train, idx_test = train_test_split(list(range(len(x_data))),
                                       stratify=y_data,
                                       test_size=TRAIN_TEST_RATIO)

# Разделение в соответствии с полученными индексами
x_train, x_test = x_data[idx_train], x_data[idx_test]
x_train_01, x_test_01 = x_data_01[idx_train], x_data_01[idx_test]
y_train, y_test = y_data[idx_train], y_data[idx_test]

# Проверка результата
print(x_train.shape, x_test.shape)
print(x_train_01.shape, x_test_01.shape)
print(y_train.shape, y_test.shape)

# Проверка сбалансированности выборок по классам
fig = plt.figure(figsize=(10, 5))
c_train = np.bincount(np.argmax(y_train, axis=1))
c_test = np.bincount(np.argmax(y_test, axis=1))
plt.bar(CLASS_LIST, c_train)
plt.bar(CLASS_LIST, c_test)
fig.autofmt_xdate(rotation=45)
plt.show()

# Последовательная модель
model_text_bow_dense = Sequential()
# Входной полносвязный слой
model_text_bow_dense.add(Dense(100, input_dim=VOCAB_SIZE, activation="relu"))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Второй полносвязный слой
model_text_bow_dense.add(Dense(100, activation='relu'))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Третий полносвязный слой
model_text_bow_dense.add(Dense(100, activation='relu'))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Выходной полносвязный слой
model_text_bow_dense.add(Dense(CLASS_COUNT, activation='softmax'))

# Входные данные подаются в виде векторов bag of words
compile_train_eval_model(model_text_bow_dense,
                         x_train_01, y_train,
                         x_test_01, y_test,
                         epochs=20,
                         batch_size=32,
                         cm_size=(16, 16),
                         class_labels=CLASS_LIST,
                         title='BoW + dense')

# Построение гистограммы распределения длин текстов в словах основного набора
seq_len = [len(x) for x in x_data]
plt.hist(seq_len, 50)
plt.show()

# Снижение размерности входных данных:
# ограничение длины последовательностей до разумного предела
seq_max_len = 200
x_train_clip = pad_sequences(x_train, maxlen=seq_max_len)
x_test_clip = pad_sequences(x_test, maxlen=seq_max_len)

# Проверка формы результата
print(x_train_clip.shape, x_test_clip.shape)

# Последовательная модель
model_text_emb_dense = Sequential()
# Cлой эмбеддингов
model_text_emb_dense.add(Embedding(VOCAB_SIZE, 50, input_length=seq_max_len))
# Слой регуляризации Dropout
model_text_emb_dense.add(SpatialDropout1D(0.2))
# Cлой преобразования многомерных данных в одномерные
model_text_emb_dense.add(Flatten())
# Слой пакетной нормализации
model_text_emb_dense.add(BatchNormalization())
# Полносвязный слой
model_text_emb_dense.add(Dense(64, activation="relu"))
# Слой регуляризации Dropout
model_text_emb_dense.add(Dropout(0.2))
# Слой пакетной нормализации
model_text_emb_dense.add(BatchNormalization())
# Выходной полносвязный слой
model_text_emb_dense.add(Dense(CLASS_COUNT, activation='softmax'))

compile_train_eval_model(model_text_emb_dense,
                         x_train_clip, y_train,
                         x_test_clip, y_test,
                         epochs=20,
                         batch_size=32,
                         cm_size=(16, 16),
                         class_labels=CLASS_LIST,
                         title='Embedding + dense')





"""## **Модель технология**"""

# Функция вывода сводки по распределению данных
def data_summary(df, Технология3):
    for cls in Технология3:
        print(f'Количество записей класса {cls}: {df[df.Технология3 == cls].shape[0]}')

# Сводка по распределению данных
data_summary(df, df.Технология3.unique())

# Задание параметров преобразования
VOCAB_SIZE       = 30000                  # Объем словаря для токенизатора
TRAIN_TEST_RATIO = 0.4                    # Доля проверочной выборки в основном наборе

# Извлечение всех записей-текстов обращений и их меток классов
text_data = df['Описание продукта (с указанием конкретных характеристик)'].tolist()
class_data = df.Технология3.tolist()

# Токенизация и построение частотного словаря по обучающим текстам
# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря
tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='неизвестное_слово', char_level=False)

# Использованы параметры:
# num_words   - объем словаря
# filters     - убираемые из текста ненужные символы
# lower       - приведение слов к нижнему регистру
# split       - разделитель слов
# char_level  - указание разделять по словам, а не по единичным символам
# oov_token   - токен для слов, которые не вошли в словарь

# Построение частотного словаря по обучающим текстам
tokenizer.fit_on_texts(text_data)

# Построение словаря в виде пар слово - индекс
items = list(tokenizer.word_index.items())

# Кодирование меток классов индексами (числами)
encoder = LabelEncoder()
class_labels = encoder.fit_transform(class_data)

CLASS_LIST = encoder.classes_
CLASS_COUNT = len(CLASS_LIST)

print(f'Размер словаря: {len(items)}')
print(f'Список классов: {CLASS_LIST}')
print(f'Всего классов: {CLASS_COUNT}')
print(f'Форма выходных данных: {class_labels.shape}')
print(f'Пример числовых меток классов: {class_labels[:10]}')

# Преобразование входных текстов в последовательности индексов (для архитектур с embedding)
# Применено приведение к массиву объектов для дальнейшего разделения на выборки
x_data = np.array(tokenizer.texts_to_sequences(text_data), dtype=object)
# Вывод формы и примера данных
print(x_data.shape) 
print(x_data[0][:20])

# Преобразование входных текстов в разреженную матрицу из векторов Bag of Words
x_data_01 = tokenizer.texts_to_matrix(text_data)
# Вывод формы и примера данных
print(x_data_01.shape) 
print(x_data_01[0, :20])

# Преобразование меток класса к векторам one hot encoding
y_data = utils.to_categorical(class_labels, CLASS_COUNT)
# Вывод формы и примера данных
print(y_data.shape)
print(y_data[0])

# Функция компиляции и обучения модели нейронной сети
def compile_train_model(model, 
                        x_train,
                        y_train,
                        x_val,
                        y_val,
                        optimizer='adam',
                        epochs=50,
                        batch_size=128,
                        figsize=(20, 5)):

    # Компиляция модели
    model.compile(optimizer=optimizer, 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])

    # Вывод сводки
    model.summary()

    # Обучение модели с заданными параметрами
    history = model.fit(x_train,
                        y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(x_val, y_val))

    # Вывод графиков точности и ошибки
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    fig.suptitle('График процесса обучения модели')
    ax1.plot(history.history['accuracy'], 
               label='Доля верных ответов на обучающем наборе')
    ax1.plot(history.history['val_accuracy'], 
               label='Доля верных ответов на проверочном наборе')
    ax1.xaxis.get_major_locator().set_params(integer=True)
    ax1.set_xlabel('Эпоха обучения')
    ax1.set_ylabel('Доля верных ответов')
    ax1.legend()

    ax2.plot(history.history['loss'], 
               label='Ошибка на обучающем наборе')
    ax2.plot(history.history['val_loss'], 
               label='Ошибка на проверочном наборе')
    ax2.xaxis.get_major_locator().set_params(integer=True)
    ax2.set_xlabel('Эпоха обучения')
    ax2.set_ylabel('Ошибка')
    ax2.legend()
    plt.show()


# Функция вывода результатов оценки модели на заданных данных
def eval_model(model, x, y_true,
               class_labels=[],
               cm_round=3,
               title='',
               figsize=(10, 10)):
    # Вычисление предсказания сети
    y_pred = model.predict(x)
    # Построение матрицы ошибок
    cm = confusion_matrix(np.argmax(y_true, axis=1),
                          np.argmax(y_pred, axis=1),
                          normalize='true')
    # Округление значений матрицы ошибок
    cm = np.around(cm, cm_round)

    # Отрисовка матрицы ошибок
    fig, ax = plt.subplots(figsize=figsize)
    ax.set_title(f'Нейросеть {title}: матрица ошибок нормализованная', fontsize=18)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
    disp.plot(ax=ax)
    plt.gca().images[-1].colorbar.remove()  # Стирание ненужной цветовой шкалы
    plt.xlabel('Предсказанные классы', fontsize=16)
    plt.ylabel('Верные классы', fontsize=16)
    fig.autofmt_xdate(rotation=45)          # Наклон меток горизонтальной оси при необходимости
    plt.show()    

    print('-'*100)
    print(f'Нейросеть: {title}')

    # Для каждого класса:
    for cls in range(len(class_labels)):
        # Определяется индекс класса с максимальным значением предсказания (уверенности)
        cls_pred = np.argmax(cm[cls])
        # Формируется сообщение о верности или неверности предсказания
        msg = 'ВЕРНО :-)' if cls_pred == cls else 'НЕВЕРНО :-('
        # Выводится текстовая информация о предсказанном классе и значении уверенности
        print('Класс: {:<20} {:3.0f}% сеть отнесла к классу {:<20} - {}'.format(class_labels[cls],
                                                                               100. * cm[cls, cls_pred],
                                                                               class_labels[cls_pred],
                                                                               msg))

    # Средняя точность распознавания определяется как среднее диагональных элементов матрицы ошибок
    print('\nСредняя точность распознавания: {:3.0f}%'.format(100. * cm.diagonal().mean()))


# Совместная функция обучения и оценки модели нейронной сети
def compile_train_eval_model(model, 
                             x_train,
                             y_train,
                             x_test,
                             y_test,
                             class_labels=CLASS_LIST,
                             title='',
                             optimizer='adam',
                             epochs=50,
                             batch_size=128,
                             graph_size=(20, 5),
                             cm_size=(10, 10)):

    # Компиляция и обучение модели на заданных параметрах
    # В качестве проверочных используются тестовые данные
    compile_train_model(model, 
                        x_train, y_train,
                        x_test, y_test,
                        optimizer=optimizer,
                        epochs=epochs,
                        batch_size=batch_size,
                        figsize=graph_size)

    # Вывод результатов оценки работы модели на тестовых данных
    eval_model(model, x_test, y_test, 
               class_labels=class_labels, 
               title=title,
               figsize=cm_size)

# Построение гистограммы распределения длин текстов основного набора
seq_len = [len(x) for x in x_data]
plt.hist(seq_len, 50)
plt.show()

# Назначение интервала допустимого количества текстов по классу
text_count_interval = [5, 200]

# Фильтрация данных - отбрасываются все данные без категории
# и все классы объемом менее нижней границы разрешенного интервала
# class_to_drop = ['нет данных']
for cls in df.Технология3.unique():
    if df[df.Технология3 == cls].shape[0] < text_count_interval[0]:
        class_to_drop.append(cls)

print(f'Удаляются классы: {class_to_drop}')

del_mask = df.Технология3.isin(class_to_drop)
df = df[~del_mask]

# По всем оставшимся классам количество примеров ограничивается не более
# верхней границы разрешенного интервала
for cls in df.Технология3.unique():
    df = df.drop(df[df.Технология3 == cls].index[text_count_interval[1]:])

data_summary(df, df.Подотрасль.unique())

# Извлечение всех записей-текстов обращений и их меток классов
text_data = df['Описание продукта (с указанием конкретных характеристик)'].tolist()
class_data = df.Подотрасль.tolist()

# Токенизация и построение частотного словаря по обучающим текстам
# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря
tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='неизвестное_слово', char_level=False)

# Использованы параметры:
# num_words   - объем словаря
# filters     - убираемые из текста ненужные символы
# lower       - приведение слов к нижнему регистру
# split       - разделитель слов
# char_level  - указание разделять по словам, а не по единичным символам
# oov_token   - токен для слов, которые не вошли в словарь

# Построение частотного словаря по обучающим текстам
tokenizer.fit_on_texts(text_data)

# Построение словаря в виде пар слово - индекс
items = list(tokenizer.word_index.items())

# Кодирование меток классов индексами (числами)
encoder = LabelEncoder()
class_labels = encoder.fit_transform(class_data)

CLASS_LIST = encoder.classes_
CLASS_COUNT = len(CLASS_LIST)

print(f'Размер словаря: {len(items)}')
print(f'Список классов: {CLASS_LIST}')
print(f'Всего классов: {CLASS_COUNT}')
print(f'Форма выходных данных: {class_labels.shape}')
print(f'Пример числовых меток классов: {class_labels[:10]}')

# Преобразование входных текстов в последовательности индексов (для архитектур с embedding)
# Применено приведение к массиву объектов для дальнейшего разделения на выборки
x_data = np.array(tokenizer.texts_to_sequences(text_data), dtype=object)
# Вывод формы и примера данных
print(x_data.shape) 
print(x_data[0][:20])

# Преобразование входных текстов в разреженную матрицу из векторов Bag of Words
x_data_01 = tokenizer.texts_to_matrix(text_data)
# Вывод формы и примера данных
print(x_data_01.shape) 
print(x_data_01[0, :20])

# Преобразование меток класса к векторам one hot encoding
y_data = utils.to_categorical(class_labels, CLASS_COUNT)
# Вывод формы и примера данных
print(y_data.shape)
print(y_data[0])

# Получение индексов разделения основного набора на обучающую и тестовую выборки
idx_train, idx_test = train_test_split(list(range(len(x_data))),
                                       stratify=y_data,
                                       test_size=TRAIN_TEST_RATIO)

# Разделение в соответствии с полученными индексами
x_train, x_test = x_data[idx_train], x_data[idx_test]
x_train_01, x_test_01 = x_data_01[idx_train], x_data_01[idx_test]
y_train, y_test = y_data[idx_train], y_data[idx_test]

# Проверка результата
print(x_train.shape, x_test.shape)
print(x_train_01.shape, x_test_01.shape)
print(y_train.shape, y_test.shape)

# Проверка сбалансированности выборок по классам
fig = plt.figure(figsize=(10, 5))
c_train = np.bincount(np.argmax(y_train, axis=1))
c_test = np.bincount(np.argmax(y_test, axis=1))
plt.bar(CLASS_LIST, c_train)
plt.bar(CLASS_LIST, c_test)
fig.autofmt_xdate(rotation=45)
plt.show()

# Последовательная модель
model_text_bow_dense = Sequential()
# Входной полносвязный слой
model_text_bow_dense.add(Dense(100, input_dim=VOCAB_SIZE, activation="relu"))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Второй полносвязный слой
model_text_bow_dense.add(Dense(100, activation='relu'))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Третий полносвязный слой
model_text_bow_dense.add(Dense(100, activation='relu'))
# Слой регуляризации Dropout
model_text_bow_dense.add(Dropout(0.4))
# Выходной полносвязный слой
model_text_bow_dense.add(Dense(CLASS_COUNT, activation='softmax'))

# Входные данные подаются в виде векторов bag of words
compile_train_eval_model(model_text_bow_dense,
                         x_train_01, y_train,
                         x_test_01, y_test,
                         epochs=20,
                         batch_size=32,
                         cm_size=(16, 16),
                         class_labels=CLASS_LIST,
                         title='BoW + dense')

# Построение гистограммы распределения длин текстов в словах основного набора
seq_len = [len(x) for x in x_data]
plt.hist(seq_len, 50)
plt.show()

# Снижение размерности входных данных:
# ограничение длины последовательностей до разумного предела
seq_max_len = 200
x_train_clip = pad_sequences(x_train, maxlen=seq_max_len)
x_test_clip = pad_sequences(x_test, maxlen=seq_max_len)

# Проверка формы результата
print(x_train_clip.shape, x_test_clip.shape)

# Последовательная модель
model_text_emb_dense = Sequential()
# Cлой эмбеддингов
model_text_emb_dense.add(Embedding(VOCAB_SIZE, 50, input_length=seq_max_len))
# Слой регуляризации Dropout
model_text_emb_dense.add(SpatialDropout1D(0.2))
# Cлой преобразования многомерных данных в одномерные
model_text_emb_dense.add(Flatten())
# Слой пакетной нормализации
model_text_emb_dense.add(BatchNormalization())
# Полносвязный слой
model_text_emb_dense.add(Dense(64, activation="relu"))
# Слой регуляризации Dropout
model_text_emb_dense.add(Dropout(0.2))
# Слой пакетной нормализации
model_text_emb_dense.add(BatchNormalization())
# Выходной полносвязный слой
model_text_emb_dense.add(Dense(CLASS_COUNT, activation='softmax'))

compile_train_eval_model(model_text_emb_dense,
                         x_train_clip, y_train,
                         x_test_clip, y_test,
                         epochs=20,
                         batch_size=32,
                         cm_size=(16, 16),
                         class_labels=CLASS_LIST,
                         title='Embedding + dense')













"""## **Модель ОКПД**"""

# Функция вывода сводки по распределению данных
def data_summary(df, Подотрасль):
    for cls in Подотрасль:
        print(f'Количество записей класса {cls}: {df[df.Подотрасль == cls].shape[0]}')



















"""## anvil"""

!pip install anvil-uplink

import anvil.server
anvil.server.connect("4JRGT4YFRHXVKN2E6DCZ236W-BMK4WG3FUTLLY6FJ")

